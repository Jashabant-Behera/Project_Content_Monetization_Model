***

# Project Case Study and Concept Guide: YouTube Content Monetization Model

This file is for learning purposes. It explains the problem, the pipeline, and every major concept with code-style examples based on the `Project_Content_Monetization_Model`.

***

## 1. Problem statement

**Problem:** We have a dataset of YouTube videos containing performance metrics (views, likes, comments, watch time) and metadata (category, device, country). We need to predict the **Ad Revenue (USD)** generated by each video.

- **Input Features (X):**
  - **Numerical:** `views`, `likes`, `comments`, `watch_time_minutes`, `video_length_minutes`, `subscribers`, `engagement_rate`.
  - **Categorical:** `category` (e.g., Gaming, Music), `device` (e.g., Mobile, TV), `country` (e.g., US, IN).
- **Output Target (y):** `ad_revenue_usd` (Continuous numerical value).
- **Business Context:**
  - **Creators:** Can estimate potential earnings before publishing or understand which metrics (e.g., Watch Time vs. Likes) drive revenue most effectively.
  - **Advertisers/Platform:** Can forecast payouts and budget accordingly.

***

## 2. Project structure and files

This project follows a modular data science structure. Here is why each file exists:

- **`src/Data/raw/`**: Contains `youtube_ad_revenue_dataset.csv`. This is the immutable original data source. We never modify this directly to ensure reproducibility.
- **`src/Data/processed/`**: Contains `youtube_ad_revenue_processed.csv`. This is the data *after* cleaning, imputation, and encoding. We save this so we don't have to re-run preprocessing every time we want to train a model.
- **`src/notebooks/EDA.ipynb`**: **Exploratory Data Analysis**. Used to understand the data's "personality" (distributions, correlations, outliers) before doing any math.
- **`src/notebooks/Preprocessing.ipynb`**: **Data Engineering**. Handles missing values, removes duplicates, creates new features (Feature Engineering), and converts text to numbers (Encoding).
- **`src/notebooks/Model_Building.ipynb`**: **Machine Learning**. Scales the data, splits it, trains multiple algorithms, evaluates them, and saves the best one.
- **`src/models/`**: Stores the "artifacts" required for the app to work:
  - `best_model.pkl`: The trained Ridge Regression model.
  - `scaler.pkl`: The StandardScaler object (to scale new user inputs).
  - `feature_names.pkl`: A list of columns to ensure the app sends data in the correct order.
- **`app.py`**: **Deployment**. A Streamlit web application that allows a non-technical user to interact with the model.
- **`requirements.txt`**: Lists all Python libraries needed to run the project.
- **`caseStudy_revised.md`**: This guide.

***

## 3. Data loading and inspection

### 3.1 Importing libraries

Here are the libraries used in `EDA.ipynb` and `Preprocessing.ipynb`:

```python
import pandas as pd       # Data manipulation (DataFrames)
import numpy as np        # Mathematical operations (arrays, sqrt)
import os                 # Operating System interface (file paths)
import matplotlib.pyplot as plt  # Basic plotting
import seaborn as sns     # Advanced statistical plotting
```

- **`pandas`**: Essential for tabular data. It lets us load CSVs (`read_csv`) and manipulate columns like Excel.
- **`numpy`**: Used for fast numerical calculations. Pandas is built on top of it.
- **`os`**: Used to create file paths that work on both Windows and Mac/Linux (e.g., `os.path.join`).
- **`seaborn`**: Used for the Correlation Heatmap. It makes matplotlib graphs prettier and easier to create.

### 3.2 Reading the dataset

```python
df = pd.read_csv(raw_data_path)
df.head()
df.info()
df.describe()
```

- **`head()`**: Shows the first 5 rows. We check if the data looks like what we expect (headers are correct, data isn't garbled).
- **`info()`**: Shows data types (int, float, object) and non-null counts. This is the first step to finding missing values.
- **`describe()`**: Shows statistics (mean, min, max). We check for weird things like negative views or a video length of 0.

***

## 4. Data cleaning and preprocessing

### 4.1 Handling missing values

In `Preprocessing.ipynb`, we found missing values in `likes`, `comments`, and `watch_time_minutes`.

```python
# Impute missing numerical values with median
numeric_cols_with_nan = ['likes', 'comments', 'watch_time_minutes']
for col in numeric_cols_with_nan:
    df[col] = df[col].fillna(df[col].median())
```

- **Strategy**: We used **Median Imputation**.
- **Why Median?**: The mean (average) is sensitive to outliers. If one video has 1 billion views, the average jumps up. The median (middle value) is robust and represents the "typical" video better.
- **Alternatives**:
  - `SimpleImputer(strategy='mean')`: Good for normal distributions.
  - `KNNImputer`: Fills missing values based on similar rows (more accurate but slower).

### 4.2 Encoding categorical variables

We converted `category`, `device`, and `country` into numbers.

```python
model_df = pd.get_dummies(model_df, columns=categorical_features, drop_first=True)
```

- **Technique**: **One-Hot Encoding** (`get_dummies`).
- **How it works**: Creates a new column for each category (e.g., `device_Mobile`, `device_TV`). 1 means yes, 0 means no.
- **`drop_first=True`**: Removes the first column (e.g., `device_Desktop`) to avoid **Multicollinearity** (Dummy Variable Trap). If it's not Mobile, Tablet, or TV, the model knows it *must* be Desktop.
- **Alternatives**:
  - `LabelEncoder`: Assigns 1, 2, 3. Bad for nominal data (Model might think 3 > 1).
  - `TargetEncoder`: Replaces category with the average target value (good for high cardinality).

### 4.3 Feature scaling

In `Model_Building.ipynb`:

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

- **Why Scaling?**: `Views` can be 1,000,000, while `Video Length` is 10. Linear models (like Ridge/Lasso) will give too much weight to `Views` just because the number is big. Scaling puts them on the same scale (mean=0, variance=1).
- **`fit_transform`**: Calculates the mean/std AND scales the data (used on Training data).
- **`transform`**: Uses the *already calculated* mean/std to scale new data (used on Test data and in `app.py`).
- **Alternatives**:
  - `MinMaxScaler`: Scales to [0, 1]. Good for image data or when you need bounded values.

***

## 5. Trainâ€“test split

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

- **Why?**: We need to simulate "future" data. If we test on the same data we trained on, the model will just memorize the answers (Overfitting).
- **`test_size=0.2`**: 20% of data is held back for testing.
- **`random_state=42`**: Ensures the split is the same every time we run the code (Reproducibility).

***

## 6. Model training

### 6.1 Choosing a model & Imports

In `Model_Building.ipynb`, we imported and tested several models. Here is why:

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
```

1.  **`LinearRegression`**:
    *   **What**: Fits a straight line.
    *   **Why**: Good baseline. Simple and interpretable.
    *   **When to use**: When relationships are linear and simple.
2.  **`Ridge` (Ridge Regression)**:
    *   **What**: Linear Regression with L2 Regularization (penalty on large coefficients).
    *   **Why**: We found high correlation between `watch_time` and `revenue`. Ridge handles multicollinearity better than standard Linear Regression.
    *   **When to use**: When you have correlated features.
3.  **`Lasso` (Lasso Regression)**:
    *   **What**: Linear Regression with L1 Regularization.
    *   **Why**: Can shrink coefficients to zero (feature selection).
    *   **When to use**: When you suspect many features are useless.
4.  **`DecisionTreeRegressor`**:
    *   **What**: Splits data based on rules (e.g., "If views > 1000...").
    *   **Why**: Captures non-linear patterns.
    *   **When to use**: When data has complex, non-linear relationships.
5.  **`RandomForestRegressor`**:
    *   **What**: Many decision trees voting together.
    *   **Why**: Very robust, handles outliers well, usually high accuracy.
    *   **When to use**: When accuracy is the priority and interpretability is secondary.

### 6.2 Predictions

```python
model.fit(X_train, y_train)   # Learn the patterns
y_pred = model.predict(X_test) # Predict on unseen data
```

- **`fit`**: The learning phase. Calculates weights/coefficients.
- **`predict`**: The inference phase. Applies weights to new inputs to generate a number.

***

## 7. Model evaluation

Since this is a **Regression** problem, we use regression metrics (not Accuracy/Precision).

```python
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print("R2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("MAE:", mean_absolute_error(y_test, y_pred))
```

- **`r2_score` (R-Squared)**:
    - **What**: How much of the variance in the target is explained by the model.
    - **Result**: ~0.95 (95%).
    - **Interpretation**: Our model is excellent at explaining revenue changes.
- **`mean_squared_error` (MSE) / RMSE**:
    - **What**: Average squared difference between predicted and actual. RMSE puts it back in dollar units.
    - **Why**: Penalizes large errors heavily.
- **`mean_absolute_error` (MAE)**:
    - **What**: Average absolute difference.
    - **Why**: Easier to explain to business stakeholders ("On average, we are off by $3").

***

## 8. Saving the model with Joblib

### 8.1 What is a `.pkl` file?

- A binary file that stores the Python object state. It allows us to "freeze" the trained model and wake it up later in the web app.

### 8.2 Saving (Serialization)

We used `joblib` instead of `pickle` because it's optimized for numpy arrays (which sklearn uses).

```python
import joblib

# Saving the best model (Ridge)
joblib.dump(best_model, 'src/models/best_model.pkl')

# Saving the scaler (CRITICAL!)
joblib.dump(scaler, 'src/models/scaler.pkl')

# Saving feature names
joblib.dump(feature_names, 'src/models/feature_names.pkl')
```

- **Why save the scaler?** The model learned 'scaled' views (e.g., 0.5). If the app sends raw views (e.g., 10,000), the model will fail. We must scale app inputs using the *exact same scaler*.

### 8.3 Loading (Deserialization)

In `app.py`:

```python
model = joblib.load('src/models/best_model.pkl')
scaler = joblib.load('src/models/scaler.pkl')
```

***

## 9. Pipelines

While we manually handled steps in this project for clarity, a **Pipeline** automates this.

```python
from sklearn.pipeline import Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge())
])
pipeline.fit(X_train, y_train)
```

- **Benefit**: You only need to save one object (`pipeline.pkl`). When you call `predict`, it automatically scales the data first. This prevents "Data Leakage" and forgetting to scale in production.

***

## 10. Notebook Walkthrough (Cell-by-Cell)

### **Notebook: `EDA.ipynb`**

**Cell: Loading Data**
```python
df = pd.read_csv(raw_data_path)
```
**Explanation:** Loads the raw CSV into memory.

**Cell: Correlation Matrix**
```python
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```
**Explanation:** Calculates the correlation coefficient (-1 to 1) between all numerical pairs.
- **Key Finding:** `ad_revenue_usd` vs `watch_time_minutes` = 0.99. This tells us Watch Time is the #1 predictor of revenue.

### **Notebook: `Preprocessing.ipynb`**

**Cell: Imputation**
```python
df[col] = df[col].fillna(df[col].median())
```
**Explanation:** Fills holes in the data. We chose median to avoid outliers skewing the data.

**Cell: Feature Engineering**
```python
df['engagement_rate'] = (df['likes'] + df['comments']) / df['views']
```
**Explanation:** Creates a new metric that represents "quality" better than raw likes.

**Cell: One-Hot Encoding**
```python
model_df = pd.get_dummies(...)
```
**Explanation:** Converts "Gaming", "Music" into `category_Gaming` (0/1) columns so the math works.

### **Notebook: `Model_Building.ipynb`**

**Cell: Scaling**
```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```
**Explanation:** Normalizes the range of features so big numbers don't dominate small numbers.

**Cell: Model Loop**
```python
for name, model in models.items():
    model.fit(X_train, y_train)
    ...
```
**Explanation:** Iterates through Linear, Ridge, Lasso, Trees, and Forest. Trains each one and prints the score. Ridge won because it handles the high correlation between features well.

**Cell: Saving**
```python
joblib.dump(best_model, ...)
```
**Explanation:** Exports the brain (model) and the translator (scaler) to the disk for `app.py` to use.

***
